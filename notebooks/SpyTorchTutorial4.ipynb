{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw82HOOThJ8m"
      },
      "source": [
        "# Tutorial 4: Training a spiking neural network on a spiking dataset (Spiking Heidelberg Digits)\n",
        "\n",
        "Manu Srinath Halvagal (https://zenkelab.org/team/) and Friedemann Zenke (https://fzenke.net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_8PinOxhJ8q"
      },
      "source": [
        "For more details on surrogate gradient learning, please see:\n",
        "\n",
        "> Neftci, E.O., Mostafa, H., and Zenke, F. (2019). Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-based optimization to spiking neural networks. IEEE Signal Process Mag 36, 51–63.\n",
        "> https://ieeexplore.ieee.org/document/8891809 and https://arxiv.org/abs/1901.09948\n",
        "\n",
        "> Cramer, B., Stradmann, Y., Schemmel, J., and Zenke, F. (2020). The Heidelberg Spiking Data Sets for the Systematic Evaluation of Spiking Neural Networks. IEEE Transactions on Neural Networks and Learning Systems 1–14.\n",
        "> https://ieeexplore.ieee.org/document/9311226 and https://arxiv.org/abs/1910.07407\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nnOHMBOhJ8t"
      },
      "source": [
        "Here we apply the model to train a spiking network to learn the Spiking Heidelberg Digits dataset (https://compneuro.net/posts/2019-spiking-heidelberg-digits/). This dataset uses a more sophisticated cochlear model to generate the spike data corresponding to audio recordings of spoken digits (examples below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hXCH-skshJ8u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import h5py\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils import data\n",
        "\n",
        "from utils import get_shd_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "foxay8kJhJ8x"
      },
      "outputs": [],
      "source": [
        "dtype = torch.float\n",
        "\n",
        "# Check whether a GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGAAnfnNhJ8y"
      },
      "source": [
        "### Setup of the spiking dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AaTLRuC2hJ8y",
        "outputId": "0eae1c15-1fd2-4585-d908-0332e0fe3436",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available at: /content/data/hdspikes/shd_train.h5\n",
            "Available at: /content/data/hdspikes/shd_test.h5\n",
            "Success! Data is mapped and ready for training.\n",
            "Training samples: 8156\n",
            "Testing samples:  2264\n"
          ]
        }
      ],
      "source": [
        "# 1. Define a clear, absolute path in the Colab/Kaggle workspace\n",
        "# Using '/content/data' for Colab or './data' for a general environment\n",
        "cache_dir = \"/content/data\"\n",
        "cache_subdir = \"hdspikes\"\n",
        "target_dir = os.path.join(cache_dir, cache_subdir)\n",
        "\n",
        "# 2. Ensure the directory exists\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# 3. Download the dataset to that specific directory\n",
        "get_shd_dataset(cache_dir, cache_subdir)\n",
        "\n",
        "# 4. Define the file paths explicitly\n",
        "train_path = os.path.join(target_dir, 'shd_train.h5')\n",
        "test_path = os.path.join(target_dir, 'shd_test.h5')\n",
        "\n",
        "# 5. Open the files and access the data\n",
        "if os.path.exists(train_path) and os.path.exists(test_path):\n",
        "    train_file = h5py.File(train_path, 'r')\n",
        "    test_file = h5py.File(test_path, 'r')\n",
        "\n",
        "    # Assigning to variables\n",
        "    # Note: These are HDF5 datasets; they act like arrays but stay on disk\n",
        "    # until you index them (e.g., x_train[0:10])\n",
        "    x_train = train_file['spikes']\n",
        "    y_train = train_file['labels']\n",
        "    x_test = test_file['spikes']\n",
        "    y_test = test_file['labels']\n",
        "\n",
        "    print(\"Success! Data is mapped and ready for training.\")\n",
        "    print(f\"Training samples: {len(y_train)}\")\n",
        "    print(f\"Testing samples:  {len(y_test)}\")\n",
        "else:\n",
        "    print(f\"Error: Files not found at {target_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAHMyDxRhJ80"
      },
      "source": [
        "The code for learning the SHD dataset is nearly identical to what we have seen for the FashionMNIST dataset in the last two tutorials. An important difference is that, now, we have the input data already in the form of spikes. This is reflected in the sparse_data_generator below.\n",
        "\n",
        "In order to use the data for learning with our spiking network, we also need to discretize the spike times into n_steps bins. Note the additional max_time argument (~1.4 for SHD) that forms the upper limit of the bins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vy0B_Ib0hJ81"
      },
      "outputs": [],
      "source": [
        "def sparse_data_generator_from_hdf5_spikes(X, y, batch_size, nb_steps, nb_units, max_time, shuffle=True):\n",
        "    labels_ = np.array(y, dtype=int)\n",
        "    number_of_batches = len(labels_) // batch_size\n",
        "    sample_index = np.arange(len(labels_))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(sample_index)\n",
        "\n",
        "    for batch_idx in range(number_of_batches):\n",
        "        batch_slice = sample_index[batch_idx * batch_size: (batch_idx + 1) * batch_size]\n",
        "\n",
        "        # Extract spike times and units for this batch\n",
        "        times = [X['times'][i] for i in batch_slice]\n",
        "        units = [X['units'][i] for i in batch_slice]\n",
        "\n",
        "        # Convert to sparse tensor format\n",
        "        # Note: We use torch.int64 for indices and torch.float32 for values\n",
        "        coo = []\n",
        "        for i, (t, u) in enumerate(zip(times, units)):\n",
        "            # Discretize time into bins\n",
        "            t_bins = np.floor(t * (nb_steps - 1) / max_time).astype(int)\n",
        "\n",
        "            # Filter spikes that fall outside the simulation window\n",
        "            idx = np.where(t_bins < nb_steps)[0]\n",
        "\n",
        "            # Create coordinate list: [batch_index, time_bin, unit_id]\n",
        "            for j in idx:\n",
        "                coo.append([i, t_bins[j], u[j]])\n",
        "\n",
        "        coo = np.array(coo)\n",
        "        i = torch.LongTensor(coo).t()\n",
        "        v = torch.FloatTensor(np.ones(len(coo)))\n",
        "\n",
        "        # Create the sparse tensor\n",
        "        # Shape: (batch_size, nb_steps, nb_units)\n",
        "        X_batch = torch.sparse_coo_tensor(i, v, torch.Size([batch_size, nb_steps, nb_units]))\n",
        "\n",
        "        y_batch = torch.tensor(labels_[batch_slice], dtype=torch.long)\n",
        "\n",
        "        yield X_batch, y_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4JqaCsshJ82"
      },
      "source": [
        "### Setup of the spiking network model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qdFQcO3hJ83"
      },
      "source": [
        "Let's also now include recurrent weights in the hidden layer. This significantly improves performance on the SHD dataset ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iM6bX1NGhJ84"
      },
      "outputs": [],
      "source": [
        "def plot_voltage_traces(mem, spk=None, dim=(3,5), spike_height=5):\n",
        "    gs=GridSpec(*dim)\n",
        "    if spk is not None:\n",
        "        dat = 1.0*mem\n",
        "        dat[spk>0.0] = spike_height\n",
        "        dat = dat.detach().cpu().numpy()\n",
        "    else:\n",
        "        dat = mem.detach().cpu().numpy()\n",
        "    for i in range(np.prod(dim)):\n",
        "        if i==0: a0=ax=plt.subplot(gs[i])\n",
        "        else: ax=plt.subplot(gs[i],sharey=a0)\n",
        "        ax.plot(dat[i])\n",
        "        ax.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bzOhKMw-hJ84"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def live_plot(loss):\n",
        "    if len(loss) == 1:\n",
        "        return\n",
        "    clear_output(wait=True)\n",
        "    ax = plt.figure(figsize=(3,2), dpi=150).gca()\n",
        "    ax.plot(range(1, len(loss) + 1), loss)\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "    ax.xaxis.get_major_locator().set_params(integer=True)\n",
        "    sns.despine()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLXMJLdbhJ85"
      },
      "source": [
        "## Training the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YWJXmTGRhJ85"
      },
      "outputs": [],
      "source": [
        "class SurrGradSpike(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Here we implement our spiking nonlinearity which also implements\n",
        "    the surrogate gradient. By subclassing torch.autograd.Function,\n",
        "    we will be able to use all of PyTorch's autograd functionality.\n",
        "    Here we use the normalized negative part of a fast sigmoid\n",
        "    as this was done in Zenke & Ganguli (2018).\n",
        "    \"\"\"\n",
        "\n",
        "    scale = 100.0 # controls steepness of surrogate gradient\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        \"\"\"\n",
        "        In the forward pass we compute a step function of the input Tensor\n",
        "        and return it. ctx is a context object that we use to stash information which\n",
        "        we need to later backpropagate our error signals. To achieve this we use the\n",
        "        ctx.save_for_backward method.\n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(input)\n",
        "        out = torch.zeros_like(input)\n",
        "        out[input > 0] = 1.0\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor we need to compute the\n",
        "        surrogate gradient of the loss with respect to the input.\n",
        "        Here we use the normalized negative part of a fast sigmoid\n",
        "        as this was done in Zenke & Ganguli (2018).\n",
        "        \"\"\"\n",
        "        input, = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
        "        return grad\n",
        "\n",
        "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
        "spike_fn  = SurrGradSpike.apply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-NERHmMhJ86"
      },
      "source": [
        "run_snn is also changed now in order to include the recurrent input in the hidden layer computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tVQ7_6dEhJ87"
      },
      "outputs": [],
      "source": [
        "def run_snn(inputs):\n",
        "    syn = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
        "    mem = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
        "\n",
        "    mem_rec = []\n",
        "    spk_rec = []\n",
        "\n",
        "    # Compute hidden layer activity\n",
        "    out = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "    h1_from_input = torch.einsum(\"abc,cd->abd\", (inputs, w1))\n",
        "    for t in range(nb_steps):\n",
        "        h1 = h1_from_input[:,t] + torch.einsum(\"ab,bc->ac\", (out, v1))\n",
        "        mthr = mem-1.0\n",
        "        out = spike_fn(mthr)\n",
        "        rst = out.detach() # We do not want to backprop through the reset\n",
        "\n",
        "        new_syn = alpha*syn +h1\n",
        "        new_mem =(beta*mem +syn)*(1.0-rst)\n",
        "\n",
        "        mem_rec.append(mem)\n",
        "        spk_rec.append(out)\n",
        "\n",
        "        mem = new_mem\n",
        "        syn = new_syn\n",
        "\n",
        "    mem_rec = torch.stack(mem_rec,dim=1)\n",
        "    spk_rec = torch.stack(spk_rec,dim=1)\n",
        "\n",
        "    # Readout layer\n",
        "    h2= torch.einsum(\"abc,cd->abd\", (spk_rec, w2))\n",
        "    flt = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
        "    out = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
        "    out_rec = [out]\n",
        "    for t in range(nb_steps):\n",
        "        new_flt = alpha*flt +h2[:,t]\n",
        "        new_out = beta*out +flt\n",
        "\n",
        "        flt = new_flt\n",
        "        out = new_out\n",
        "\n",
        "        out_rec.append(out)\n",
        "\n",
        "    out_rec = torch.stack(out_rec,dim=1)\n",
        "    other_recs = [mem_rec, spk_rec]\n",
        "    return out_rec, other_recs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the Loss Function\n",
        "# CrossEntropyLoss is standard for classification tasks\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 2. Define Simulation Hyperparameters\n",
        "# These values are standard for the SHD dataset tutorial\n",
        "time_step = 1e-3\n",
        "tau_mem = 10e-3\n",
        "tau_syn = 5e-3\n",
        "\n",
        "# Decay constants for the LIF neurons\n",
        "alpha = np.exp(-time_step/tau_syn)\n",
        "beta  = np.exp(-time_step/tau_mem)\n",
        "\n",
        "# Dataset/Network dimensions\n",
        "nb_inputs  = 700\n",
        "nb_hidden  = 128\n",
        "nb_outputs = 20\n",
        "nb_steps   = 100\n",
        "batch_size = 256\n",
        "max_time   = 1.0\n",
        "\n",
        "print(f\"Hyperparameters set: alpha={alpha:.4f}, beta={beta:.4f}\")"
      ],
      "metadata": {
        "id": "mLQys_w-lN9y",
        "outputId": "87c37a02-010f-4a33-a95f-f2597541b3c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters set: alpha=0.8187, beta=0.9048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jb6Uy2JihJ88"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.float\n",
        "\n",
        "# 2. Initialize Weights (Standard for Tutorial 4)\n",
        "# nb_inputs = 700 (SHD channels), nb_hidden = 128, nb_outputs = 20 (digits)\n",
        "w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w1, mean=0.0, std=1.0/np.sqrt(nb_inputs))\n",
        "\n",
        "w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w2, mean=0.0, std=1.0/np.sqrt(nb_hidden))\n",
        "\n",
        "# Recurrent weights for the hidden layer\n",
        "v1 = torch.empty((nb_hidden, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(v1, mean=0.0, std=1.0/np.sqrt(nb_hidden))\n",
        "\n",
        "# 3. Group them into the missing 'params' list\n",
        "params = [w1, w2, v1]\n",
        "\n",
        "def train(x_data, y_data, lr=2e-4, nb_epochs=10):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize your optimizer with your model parameters\n",
        "    # (Assuming params is a list of your weights [w1, w2, v1, etc.])\n",
        "    optimizer = torch.optim.Adam(params, lr=lr, betas=(0.9, 0.999))\n",
        "\n",
        "    loss_hist = []\n",
        "    for e in range(nb_epochs):\n",
        "        local_loss = []\n",
        "        for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size, nb_steps, nb_inputs, max_time):\n",
        "            # 1. Convert sparse to dense\n",
        "            # 2. Move the input tensor to the GPU\n",
        "            # 3. Move the labels to the GPU\n",
        "            x_dense = x_local.to_dense().to(device)\n",
        "            y_local = y_local.to(device)\n",
        "\n",
        "            output, recs = run_snn(x_dense)\n",
        "\n",
        "            # Loss calculation\n",
        "            # (Ensure your loss function handles the device correctly)\n",
        "            _, spks = recs\n",
        "            m, _ = torch.max(output, 1)\n",
        "            loss_val = loss_fn(m, y_local) # loss_fn should be defined earlier (e.g., nn.CrossEntropyLoss)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss_val.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            local_loss.append(loss_val.item())\n",
        "\n",
        "        mean_loss = np.mean(local_loss)\n",
        "        print(f\"Epoch {e}: loss={mean_loss:.5f}\")\n",
        "        loss_hist.append(mean_loss)\n",
        "\n",
        "    return loss_hist\n",
        "\n",
        "\n",
        "def compute_classification_accuracy(x_data, y_data):\n",
        "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
        "    accs = []\n",
        "    # Ensure we use the same device as the weights\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size, nb_steps, nb_inputs, max_time, shuffle=False):\n",
        "        inputs = x_local.to_dense().to(device)\n",
        "        output, _ = run_snn(inputs)\n",
        "\n",
        "        # Max over time dimension\n",
        "        m, _ = torch.max(output, 1)\n",
        "        # Argmax over output units\n",
        "        _, am = torch.max(m, 1)\n",
        "\n",
        "        # Compare to labels (move labels to CPU for numpy comparison)\n",
        "        tmp = np.mean((y_local.to(device) == am).detach().cpu().numpy())\n",
        "        accs.append(tmp)\n",
        "\n",
        "    return np.mean(accs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZTGY3U7_hJ89"
      },
      "outputs": [],
      "source": [
        "nb_epochs = 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xwv2FK9hJ89"
      },
      "source": [
        "WARNING: Training for a large number of epochs could take a significant amount of time. Reduce the nb_epochs parameter as necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YjwUwMdQhJ8-",
        "outputId": "dea98e3e-1d32-450c-ff2f-287659e7ec48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss=12.41592\n",
            "Epoch 1: loss=5.54596\n",
            "Epoch 2: loss=4.31861\n",
            "Epoch 3: loss=3.53309\n",
            "Epoch 4: loss=3.02597\n",
            "Epoch 5: loss=2.81757\n",
            "Epoch 6: loss=2.61801\n",
            "Epoch 7: loss=2.50400\n",
            "Epoch 8: loss=2.36878\n",
            "Epoch 9: loss=2.29585\n",
            "Epoch 10: loss=2.24258\n",
            "Epoch 11: loss=2.16660\n",
            "Epoch 12: loss=2.08771\n",
            "Epoch 13: loss=2.02664\n",
            "Epoch 14: loss=1.95409\n",
            "Epoch 15: loss=1.92637\n",
            "Epoch 16: loss=1.85362\n",
            "Epoch 17: loss=1.77021\n",
            "Epoch 18: loss=1.75679\n",
            "Epoch 19: loss=1.67732\n",
            "Epoch 20: loss=1.61864\n",
            "Epoch 21: loss=1.61235\n",
            "Epoch 22: loss=1.55574\n",
            "Epoch 23: loss=1.53585\n",
            "Epoch 24: loss=1.49590\n"
          ]
        }
      ],
      "source": [
        "loss_hist = train(x_train, y_train, lr=2e-4, nb_epochs=nb_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Y6Kk2fMBhJ8_",
        "outputId": "f1657f19-2ca7-4421-e501-aa8d50497d18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 0.529\n",
            "Test accuracy: 0.415\n"
          ]
        }
      ],
      "source": [
        "print(\"Training accuracy: %.3f\"%(compute_classification_accuracy(x_train,y_train)))\n",
        "print(\"Test accuracy: %.3f\"%(compute_classification_accuracy(x_test,y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "05CVGjOJhJ8_"
      },
      "outputs": [],
      "source": [
        "def get_mini_batch(x_data, y_data, shuffle=False):\n",
        "    for ret in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size, nb_steps, nb_inputs, max_time, shuffle=shuffle):\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hnDZD4xohJ8_",
        "outputId": "fb91a5e2-ff67-4642-a7d2-3fa44aadde18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_bmm)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2941975980.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_recordings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_snn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmem_rec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspk_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother_recordings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2751684168.py\u001b[0m in \u001b[0;36mrun_snn\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Compute hidden layer activity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mh1_from_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"abc,cd->abd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh1_from_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ab,bc->ac\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;31m# recurse in case operands contains value that has torch function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;31m# in the original implementation this line is omitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_einsum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;31m# the path for contracting 0 or 1 time(s) is already optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;31m# or the user has disabled using opt_einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but got mat2 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_bmm)"
          ]
        }
      ],
      "source": [
        "x_batch, y_batch = get_mini_batch(x_test, y_test)\n",
        "output, other_recordings = run_snn(x_batch.to_dense())\n",
        "mem_rec, spk_rec = other_recordings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as_yw6V1hJ9A"
      },
      "outputs": [],
      "source": [
        "fig=plt.figure(dpi=100)\n",
        "plot_voltage_traces(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j4rshSPhJ9A"
      },
      "outputs": [],
      "source": [
        "# Let's plot the hiddden layer spiking activity for some input stimuli\n",
        "nb_plt = 4\n",
        "gs = GridSpec(1,nb_plt)\n",
        "fig= plt.figure(figsize=(7,3),dpi=150)\n",
        "for i in range(nb_plt):\n",
        "    plt.subplot(gs[i])\n",
        "    plt.imshow(spk_rec[i].detach().cpu().numpy().T,cmap=plt.cm.gray_r, origin=\"lower\" )\n",
        "    if i==0:\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"Units\")\n",
        "\n",
        "    sns.despine()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_history(loss_hist):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(loss_hist, label=\"Training Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"SNN Training Progress on SHD\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_history(loss_hist)"
      ],
      "metadata": {
        "id": "Q4Ydt5ICjurv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_spike_raster(batch_idx=0):\n",
        "    # Get a single sample from the test set\n",
        "    x_local, y_local = next(iter(sparse_data_generator_from_hdf5_spikes(x_test, y_test, batch_size, nb_steps, nb_inputs, max_time)))\n",
        "    output, recs = run_snn(x_local.to_dense().to(device))\n",
        "    mem_rec, spk_rec = recs\n",
        "\n",
        "    # spk_rec shape is (batch, time, neurons)\n",
        "    # Pick the first sample in the batch and move to CPU\n",
        "    spikes = spk_rec[batch_idx].detach().cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    # Find coordinates of spikes (where value is 1)\n",
        "    t_idx, n_idx = np.where(spikes > 0)\n",
        "    plt.scatter(t_idx, n_idx, s=2, c=\"black\", marker=\"|\")\n",
        "    plt.xlabel(\"Time Step\")\n",
        "    plt.ylabel(\"Neuron Index\")\n",
        "    plt.title(f\"Hidden Layer Spike Raster (Label: {y_local[batch_idx]})\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_spike_raster()"
      ],
      "metadata": {
        "id": "ALxsPfN-j2e0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_membrane_potentials(batch_idx=0, neuron_idx=0):\n",
        "    x_local, y_local = next(iter(sparse_data_generator_from_hdf5_spikes(x_test, y_test, batch_size, nb_steps, nb_inputs, max_time)))\n",
        "    output, recs = run_snn(x_local.to_dense().to(device))\n",
        "    mem_rec, _ = recs\n",
        "\n",
        "    # Extract potential for one specific neuron over time\n",
        "    potentials = mem_rec[batch_idx, :, neuron_idx].detach().cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(potentials, label=f\"Neuron {neuron_idx}\")\n",
        "    plt.axhline(y=1.0, color='r', linestyle='--', label=\"Threshold\")\n",
        "    plt.xlabel(\"Time Step\")\n",
        "    plt.ylabel(\"Membrane Potential (V)\")\n",
        "    plt.title(\"LIF Neuron Membrane Dynamics\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_membrane_potentials(neuron_idx=50)"
      ],
      "metadata": {
        "id": "q0vP900tj5lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykD5ZzBohJ9A"
      },
      "source": [
        "We see that spiking in the hidden layer is quite sparse as in the previous Tutorial 3 because we used the same activity regularizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVQohnjKhJ9B"
      },
      "source": [
        "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}