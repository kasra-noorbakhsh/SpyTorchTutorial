{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw82HOOThJ8m"
      },
      "source": [
        "# Tutorial 4: Training a spiking neural network on a spiking dataset (Spiking Heidelberg Digits)\n",
        "\n",
        "Manu Srinath Halvagal (https://zenkelab.org/team/) and Friedemann Zenke (https://fzenke.net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_8PinOxhJ8q"
      },
      "source": [
        "For more details on surrogate gradient learning, please see:\n",
        "\n",
        "> Neftci, E.O., Mostafa, H., and Zenke, F. (2019). Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-based optimization to spiking neural networks. IEEE Signal Process Mag 36, 51–63.\n",
        "> https://ieeexplore.ieee.org/document/8891809 and https://arxiv.org/abs/1901.09948\n",
        "\n",
        "> Cramer, B., Stradmann, Y., Schemmel, J., and Zenke, F. (2020). The Heidelberg Spiking Data Sets for the Systematic Evaluation of Spiking Neural Networks. IEEE Transactions on Neural Networks and Learning Systems 1–14.\n",
        "> https://ieeexplore.ieee.org/document/9311226 and https://arxiv.org/abs/1910.07407\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nnOHMBOhJ8t"
      },
      "source": [
        "In Tutorials 2 and 3, we have seen how to train a multi-layer spiking neural network on the [Fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist) along with an activity regularizer. However, the spiking activity input to the network was generated using a simple time-to-first-spike code. Here we apply the model to train a spiking network to learn the Spiking Heidelberg Digits dataset (https://compneuro.net/posts/2019-spiking-heidelberg-digits/). This dataset uses a more sophisticated cochlear model to generate the spike data corresponding to audio recordings of spoken digits (examples below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "hXCH-skshJ8u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import h5py\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils import data\n",
        "\n",
        "from utils import get_shd_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "foxay8kJhJ8x"
      },
      "outputs": [],
      "source": [
        "dtype = torch.float\n",
        "\n",
        "# Check whether a GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGAAnfnNhJ8y"
      },
      "source": [
        "### Setup of the spiking dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "AaTLRuC2hJ8y",
        "outputId": "e82bbd05-5459-44e3-c496-24ff7e8a892d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available at: /content/data/hdspikes/shd_train.h5\n",
            "Available at: /content/data/hdspikes/shd_test.h5\n",
            "Success! Data is mapped and ready for training.\n",
            "Training samples: 8156\n",
            "Testing samples:  2264\n"
          ]
        }
      ],
      "source": [
        "# 1. Define a clear, absolute path in the Colab/Kaggle workspace\n",
        "# Using '/content/data' for Colab or './data' for a general environment\n",
        "cache_dir = \"/content/data\"\n",
        "cache_subdir = \"hdspikes\"\n",
        "target_dir = os.path.join(cache_dir, cache_subdir)\n",
        "\n",
        "# 2. Ensure the directory exists\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# 3. Download the dataset to that specific directory\n",
        "# Note: get_shd_dataset usually handles the download if files aren't found\n",
        "get_shd_dataset(cache_dir, cache_subdir)\n",
        "\n",
        "# 4. Define the file paths explicitly\n",
        "train_path = os.path.join(target_dir, 'shd_train.h5')\n",
        "test_path = os.path.join(target_dir, 'shd_test.h5')\n",
        "\n",
        "# 5. Open the files and access the data\n",
        "if os.path.exists(train_path) and os.path.exists(test_path):\n",
        "    train_file = h5py.File(train_path, 'r')\n",
        "    test_file = h5py.File(test_path, 'r')\n",
        "\n",
        "    # Assigning to variables\n",
        "    # Note: These are HDF5 datasets; they act like arrays but stay on disk\n",
        "    # until you index them (e.g., x_train[0:10])\n",
        "    x_train = train_file['spikes']\n",
        "    y_train = train_file['labels']\n",
        "    x_test = test_file['spikes']\n",
        "    y_test = test_file['labels']\n",
        "\n",
        "    print(\"Success! Data is mapped and ready for training.\")\n",
        "    print(f\"Training samples: {len(y_train)}\")\n",
        "    print(f\"Testing samples:  {len(y_test)}\")\n",
        "else:\n",
        "    print(f\"Error: Files not found at {target_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAHMyDxRhJ80"
      },
      "source": [
        "The code for learning the SHD dataset is nearly identical to what we have seen for the FashionMNIST dataset in the last two tutorials. An important difference is that, now, we have the input data already in the form of spikes. This is reflected in the sparse_data_generator below.\n",
        "\n",
        "In order to use the data for learning with our spiking network, we also need to discretize the spike times into n_steps bins. Note the additional max_time argument (~1.4 for SHD) that forms the upper limit of the bins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "vy0B_Ib0hJ81"
      },
      "outputs": [],
      "source": [
        "def sparse_data_generator_from_hdf5_spikes(X, y, batch_size, nb_steps, nb_units, max_time, shuffle=True):\n",
        "    \"\"\"\n",
        "    Revised generator to fix the np.int error and handle HDF5 spike data.\n",
        "    \"\"\"\n",
        "    # FIX: Changed np.int to int\n",
        "    labels_ = np.array(y, dtype=int)\n",
        "    number_of_batches = len(labels_) // batch_size\n",
        "    sample_index = np.arange(len(labels_))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(sample_index)\n",
        "\n",
        "    for batch_idx in range(number_of_batches):\n",
        "        batch_slice = sample_index[batch_idx * batch_size: (batch_idx + 1) * batch_size]\n",
        "\n",
        "        # Extract spike times and units for this batch\n",
        "        times = [X['times'][i] for i in batch_slice]\n",
        "        units = [X['units'][i] for i in batch_slice]\n",
        "\n",
        "        # Convert to sparse tensor format\n",
        "        # Note: We use torch.int64 for indices and torch.float32 for values\n",
        "        coo = []\n",
        "        for i, (t, u) in enumerate(zip(times, units)):\n",
        "            # Discretize time into bins\n",
        "            t_bins = np.floor(t * (nb_steps - 1) / max_time).astype(int)\n",
        "\n",
        "            # Filter spikes that fall outside the simulation window\n",
        "            idx = np.where(t_bins < nb_steps)[0]\n",
        "\n",
        "            # Create coordinate list: [batch_index, time_bin, unit_id]\n",
        "            for j in idx:\n",
        "                coo.append([i, t_bins[j], u[j]])\n",
        "\n",
        "        coo = np.array(coo)\n",
        "        i = torch.LongTensor(coo).t()\n",
        "        v = torch.FloatTensor(np.ones(len(coo)))\n",
        "\n",
        "        # Create the sparse tensor\n",
        "        # Shape: (batch_size, nb_steps, nb_units)\n",
        "        X_batch = torch.sparse_coo_tensor(i, v, torch.Size([batch_size, nb_steps, nb_units]))\n",
        "\n",
        "        y_batch = torch.tensor(labels_[batch_slice], dtype=torch.long)\n",
        "\n",
        "        yield X_batch, y_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4JqaCsshJ82"
      },
      "source": [
        "### Setup of the spiking network model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qdFQcO3hJ83"
      },
      "source": [
        "Let's also now include recurrent weights in the hidden layer. This significantly improves performance on the SHD dataset ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "iM6bX1NGhJ84"
      },
      "outputs": [],
      "source": [
        "def plot_voltage_traces(mem, spk=None, dim=(3,5), spike_height=5):\n",
        "    gs=GridSpec(*dim)\n",
        "    if spk is not None:\n",
        "        dat = 1.0*mem\n",
        "        dat[spk>0.0] = spike_height\n",
        "        dat = dat.detach().cpu().numpy()\n",
        "    else:\n",
        "        dat = mem.detach().cpu().numpy()\n",
        "    for i in range(np.prod(dim)):\n",
        "        if i==0: a0=ax=plt.subplot(gs[i])\n",
        "        else: ax=plt.subplot(gs[i],sharey=a0)\n",
        "        ax.plot(dat[i])\n",
        "        ax.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "bzOhKMw-hJ84"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def live_plot(loss):\n",
        "    if len(loss) == 1:\n",
        "        return\n",
        "    clear_output(wait=True)\n",
        "    ax = plt.figure(figsize=(3,2), dpi=150).gca()\n",
        "    ax.plot(range(1, len(loss) + 1), loss)\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "    ax.xaxis.get_major_locator().set_params(integer=True)\n",
        "    sns.despine()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLXMJLdbhJ85"
      },
      "source": [
        "## Training the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "YWJXmTGRhJ85"
      },
      "outputs": [],
      "source": [
        "class SurrGradSpike(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Here we implement our spiking nonlinearity which also implements\n",
        "    the surrogate gradient. By subclassing torch.autograd.Function,\n",
        "    we will be able to use all of PyTorch's autograd functionality.\n",
        "    Here we use the normalized negative part of a fast sigmoid\n",
        "    as this was done in Zenke & Ganguli (2018).\n",
        "    \"\"\"\n",
        "\n",
        "    scale = 100.0 # controls steepness of surrogate gradient\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        \"\"\"\n",
        "        In the forward pass we compute a step function of the input Tensor\n",
        "        and return it. ctx is a context object that we use to stash information which\n",
        "        we need to later backpropagate our error signals. To achieve this we use the\n",
        "        ctx.save_for_backward method.\n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(input)\n",
        "        out = torch.zeros_like(input)\n",
        "        out[input > 0] = 1.0\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor we need to compute the\n",
        "        surrogate gradient of the loss with respect to the input.\n",
        "        Here we use the normalized negative part of a fast sigmoid\n",
        "        as this was done in Zenke & Ganguli (2018).\n",
        "        \"\"\"\n",
        "        input, = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
        "        return grad\n",
        "\n",
        "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
        "spike_fn  = SurrGradSpike.apply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-NERHmMhJ86"
      },
      "source": [
        "run_snn is also changed now in order to include the recurrent input in the hidden layer computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "tVQ7_6dEhJ87"
      },
      "outputs": [],
      "source": [
        "def run_snn(inputs):\n",
        "    syn = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
        "    mem = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
        "\n",
        "    mem_rec = []\n",
        "    spk_rec = []\n",
        "\n",
        "    # Compute hidden layer activity\n",
        "    out = torch.zeros((batch_size, nb_hidden), device=device, dtype=dtype)\n",
        "    h1_from_input = torch.einsum(\"abc,cd->abd\", (inputs, w1))\n",
        "    for t in range(nb_steps):\n",
        "        h1 = h1_from_input[:,t] + torch.einsum(\"ab,bc->ac\", (out, v1))\n",
        "        mthr = mem-1.0\n",
        "        out = spike_fn(mthr)\n",
        "        rst = out.detach() # We do not want to backprop through the reset\n",
        "\n",
        "        new_syn = alpha*syn +h1\n",
        "        new_mem =(beta*mem +syn)*(1.0-rst)\n",
        "\n",
        "        mem_rec.append(mem)\n",
        "        spk_rec.append(out)\n",
        "\n",
        "        mem = new_mem\n",
        "        syn = new_syn\n",
        "\n",
        "    mem_rec = torch.stack(mem_rec,dim=1)\n",
        "    spk_rec = torch.stack(spk_rec,dim=1)\n",
        "\n",
        "    # Readout layer\n",
        "    h2= torch.einsum(\"abc,cd->abd\", (spk_rec, w2))\n",
        "    flt = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
        "    out = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
        "    out_rec = [out]\n",
        "    for t in range(nb_steps):\n",
        "        new_flt = alpha*flt +h2[:,t]\n",
        "        new_out = beta*out +flt\n",
        "\n",
        "        flt = new_flt\n",
        "        out = new_out\n",
        "\n",
        "        out_rec.append(out)\n",
        "\n",
        "    out_rec = torch.stack(out_rec,dim=1)\n",
        "    other_recs = [mem_rec, spk_rec]\n",
        "    return out_rec, other_recs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the Loss Function\n",
        "# CrossEntropyLoss is standard for classification tasks\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 2. Define Simulation Hyperparameters\n",
        "# These values are standard for the SHD dataset tutorial\n",
        "time_step = 1e-3\n",
        "tau_mem = 10e-3\n",
        "tau_syn = 5e-3\n",
        "\n",
        "# Decay constants for the LIF neurons\n",
        "alpha = np.exp(-time_step/tau_syn)\n",
        "beta  = np.exp(-time_step/tau_mem)\n",
        "\n",
        "# Dataset/Network dimensions\n",
        "nb_inputs  = 700\n",
        "nb_hidden  = 128\n",
        "nb_outputs = 20\n",
        "nb_steps   = 100\n",
        "batch_size = 256\n",
        "max_time   = 1.0\n",
        "\n",
        "print(f\"Hyperparameters set: alpha={alpha:.4f}, beta={beta:.4f}\")"
      ],
      "metadata": {
        "id": "mLQys_w-lN9y",
        "outputId": "c454eda3-374c-440a-8d6a-e8c8d63e189a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters set: alpha=0.8187, beta=0.9048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "jb6Uy2JihJ88"
      },
      "outputs": [],
      "source": [
        "# 1. Define Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.float\n",
        "\n",
        "# 2. Initialize Weights (Standard for Tutorial 4)\n",
        "# nb_inputs = 700 (SHD channels), nb_hidden = 128, nb_outputs = 20 (digits)\n",
        "w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w1, mean=0.0, std=1.0/np.sqrt(nb_inputs))\n",
        "\n",
        "w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w2, mean=0.0, std=1.0/np.sqrt(nb_hidden))\n",
        "\n",
        "# Recurrent weights for the hidden layer\n",
        "v1 = torch.empty((nb_hidden, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(v1, mean=0.0, std=1.0/np.sqrt(nb_hidden))\n",
        "\n",
        "# 3. Group them into the missing 'params' list\n",
        "params = [w1, w2, v1]\n",
        "\n",
        "def train(x_data, y_data, lr=2e-4, nb_epochs=10):\n",
        "    # Ensure device is defined (usually 'cuda' if available, else 'cpu')\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize your optimizer with your model parameters\n",
        "    # (Assuming params is a list of your weights [w1, w2, v1, etc.])\n",
        "    optimizer = torch.optim.Adam(params, lr=lr, betas=(0.9, 0.999))\n",
        "\n",
        "    loss_hist = []\n",
        "    for e in range(nb_epochs):\n",
        "        local_loss = []\n",
        "        for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size, nb_steps, nb_inputs, max_time):\n",
        "\n",
        "            # --- FIX STARTS HERE ---\n",
        "            # 1. Convert sparse to dense\n",
        "            # 2. Move the input tensor to the GPU\n",
        "            # 3. Move the labels to the GPU\n",
        "            x_dense = x_local.to_dense().to(device)\n",
        "            y_local = y_local.to(device)\n",
        "            # --- FIX ENDS HERE ---\n",
        "\n",
        "            output, recs = run_snn(x_dense)\n",
        "\n",
        "            # Loss calculation\n",
        "            # (Ensure your loss function handles the device correctly)\n",
        "            _, spks = recs\n",
        "            m, _ = torch.max(output, 1)\n",
        "            loss_val = loss_fn(m, y_local) # loss_fn should be defined earlier (e.g., nn.CrossEntropyLoss)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss_val.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            local_loss.append(loss_val.item())\n",
        "\n",
        "        mean_loss = np.mean(local_loss)\n",
        "        print(f\"Epoch {e}: loss={mean_loss:.5f}\")\n",
        "        loss_hist.append(mean_loss)\n",
        "\n",
        "    return loss_hist\n",
        "\n",
        "\n",
        "def compute_classification_accuracy(x_data, y_data):\n",
        "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
        "    accs = []\n",
        "    # Ensure we use the same device as the weights\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    for x_local, y_local in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size, nb_steps, nb_inputs, max_time, shuffle=False):\n",
        "        # FIX: Move input to device\n",
        "        inputs = x_local.to_dense().to(device)\n",
        "        output, _ = run_snn(inputs)\n",
        "\n",
        "        # Max over time dimension\n",
        "        m, _ = torch.max(output, 1)\n",
        "        # Argmax over output units\n",
        "        _, am = torch.max(m, 1)\n",
        "\n",
        "        # Compare to labels (move labels to CPU for numpy comparison)\n",
        "        tmp = np.mean((y_local.to(device) == am).detach().cpu().numpy())\n",
        "        accs.append(tmp)\n",
        "\n",
        "    return np.mean(accs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "ZTGY3U7_hJ89"
      },
      "outputs": [],
      "source": [
        "nb_epochs = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xwv2FK9hJ89"
      },
      "source": [
        "WARNING: Training for a large number of epochs could take a significant amount of time. Reduce the nb_epochs parameter as necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjwUwMdQhJ8-",
        "outputId": "deb2c7cb-8d93-420e-9863-ebf7ecc19cfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss=13.59895\n",
            "Epoch 1: loss=5.29185\n",
            "Epoch 2: loss=3.81655\n",
            "Epoch 3: loss=3.25132\n",
            "Epoch 4: loss=2.94750\n"
          ]
        }
      ],
      "source": [
        "loss_hist = train(x_train, y_train, lr=2e-4, nb_epochs=nb_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6Kk2fMBhJ8_"
      },
      "outputs": [],
      "source": [
        "print(\"Training accuracy: %.3f\"%(compute_classification_accuracy(x_train,y_train)))\n",
        "print(\"Test accuracy: %.3f\"%(compute_classification_accuracy(x_test,y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05CVGjOJhJ8_"
      },
      "outputs": [],
      "source": [
        "def get_mini_batch(x_data, y_data, shuffle=False):\n",
        "    for ret in sparse_data_generator_from_hdf5_spikes(x_data, y_data, batch_size, nb_steps, nb_inputs, max_time, shuffle=shuffle):\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnDZD4xohJ8_"
      },
      "outputs": [],
      "source": [
        "x_batch, y_batch = get_mini_batch(x_test, y_test)\n",
        "output, other_recordings = run_snn(x_batch.to_dense())\n",
        "mem_rec, spk_rec = other_recordings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as_yw6V1hJ9A"
      },
      "outputs": [],
      "source": [
        "fig=plt.figure(dpi=100)\n",
        "plot_voltage_traces(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j4rshSPhJ9A"
      },
      "outputs": [],
      "source": [
        "# Let's plot the hiddden layer spiking activity for some input stimuli\n",
        "\n",
        "nb_plt = 4\n",
        "gs = GridSpec(1,nb_plt)\n",
        "fig= plt.figure(figsize=(7,3),dpi=150)\n",
        "for i in range(nb_plt):\n",
        "    plt.subplot(gs[i])\n",
        "    plt.imshow(spk_rec[i].detach().cpu().numpy().T,cmap=plt.cm.gray_r, origin=\"lower\" )\n",
        "    if i==0:\n",
        "        plt.xlabel(\"Time\")\n",
        "        plt.ylabel(\"Units\")\n",
        "\n",
        "    sns.despine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykD5ZzBohJ9A"
      },
      "source": [
        "We see that spiking in the hidden layer is quite sparse as in the previous Tutorial 3 because we used the same activity regularizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVQohnjKhJ9B"
      },
      "source": [
        "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Creative Commons Attribution 4.0 International License</a>."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}